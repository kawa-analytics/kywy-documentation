{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e02151",
   "metadata": {},
   "source": [
    "# Data source management and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311620c-c104-41d3-b966-cdc4be259b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kywy.client.kawa_client import KawaClient as K\n",
    "\n",
    "kawa = K.load_client_from_environment()\n",
    "cmd = kawa.commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c0806f-728f-411a-8226-47e573247106",
   "metadata": {},
   "source": [
    "## 1. Create a datasource \n",
    "\n",
    "Creating datasources in KAWA requires a data loader. \n",
    "In order to create a data loader, use the new_data_loader method on the KAWA client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7a9ca-fa95-4d22-bd59-93f42d1035cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "       'id': 1,\n",
    "       'flag':True,\n",
    "       'comment':'bar',\n",
    "       'price': 1.124,\n",
    "       'order_date': datetime.date(2035,1,1),\n",
    "}])\n",
    "\n",
    "# A data loader will be created, using the dataframe to understand what indicators\n",
    "# will constitute the datasource.\n",
    "# Here: id will be an integer, flag will be a boolean indicator, comment a text and price a decimal,\n",
    "# order_date will be a date indicator\n",
    "loader = kawa.new_data_loader(datasource_name='Sample Datasource 2', df=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87989eab-a274-4576-bc2f-670e36d3e51d",
   "metadata": {},
   "source": [
    "Once the loader is created, use it to create a datasource. \n",
    "If a datasource with the same already exists, this command won’t have any effect. \n",
    "\n",
    "It won’t update the datasource if indicators are different.\n",
    "\n",
    "__Primary keys__\n",
    "\n",
    "You can also specify the primary keys of your datasource when you create it by passing a `primary_keys` argument.\n",
    "If you omit it, KAWA will add to your dataframe (And your datasource) a new indicator: `record_id` that will be automatically\n",
    "incremented to serve as a technical primary key.\n",
    "\n",
    "---\n",
    "**ℹ️ NOTE**\n",
    "\n",
    "The primary keys will be in the same order as the columns of the dataframe, not in the order of the `primary_keys` array. Please refer to the paragraph about indexation to learn more about how to order your primary keys.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d51c8b-7bfa-4718-b849-134df9734615",
   "metadata": {},
   "source": [
    "Below are the supported types and their mapped type in KAWA.\n",
    "\n",
    "__If a column of the dataframe has an unrecognised type, the datasource cannot be built.__\n",
    "\n",
    "| Pandas type(s) | Kawa Type | Notes |\n",
    "| --- | --- | --- |\n",
    "| int, int8, int16, int32, int64, uint8, uint16, uint32, uint64 | integer |  |\n",
    "| float, float16, float32, float64 | decimal | |\n",
    "| date | date | |\n",
    "| datetime, pd.Timestamp | date_time | If the timezone is not specified, will use the local timezone. |\n",
    "| string | text | |\n",
    "| boolean | boolean | |\n",
    "| array of texts | list(text) | |\n",
    "| array of integers | list(integer) | |\n",
    "| array of floats | list(decimal) | |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894e713-2c64-4070-ba2f-b11de9fc45d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates a datasource based on the schema of the dataframe, with the given name.\n",
    "# This operation is idempotent, based on the data source name.\n",
    "# NO DATA WILL BE INSERTED AT THAT POINT\n",
    "loader.create_datasource()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c52b46-11ee-4e7d-ae26-701b868d7def",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Adding columns to an existing datasource\n",
    "In order to add indicators to an existing datasource, create a new loader with a dataframe containing the new indicators and call the `add_new_indicators_to_datasource` method. \n",
    "\n",
    "This method won’t have any effect if no new indicator is present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43996fe1-9a4b-4560-9b6e-533174ca247c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates a new data loader on an existing datasource\n",
    "df_with_new_client_indicator = pd.DataFrame([{\n",
    "       'id': 1,\n",
    "       'flag':True,\n",
    "       'comment':'bar',\n",
    "       'price': 1.124,\n",
    "       'order_date': datetime.date(2035,1,1),\n",
    "       'client':'Wayne Enterprises'\n",
    "}])\n",
    "\n",
    "\n",
    "loader = kawa.new_data_loader(\n",
    "    datasource_name='Sample Datasource',\n",
    "    df=df_with_new_client_indicator,\n",
    ")\n",
    "\n",
    "# The 'client' indicator was not part of the datasource,\n",
    "# it will be added as a text indicator\n",
    "loader.add_new_indicators_to_datasource();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ca66a-91bd-4048-8630-31db20634de8",
   "metadata": {},
   "source": [
    "## 3. Loading data into KAWA\n",
    "\n",
    "Loading data into a datasource requires a data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4699154-f450-43ec-aa7c-c1b957600402",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "#### Incremental loads\n",
    "\n",
    "This is driven by the `reset_before_insert` parameter. By default, this is `False`.\n",
    "\n",
    "If set to `False`, the data of the new dataframe will be added on top of what was there before. __This is an incremental load__. \n",
    "\n",
    "If some primary keys are defined, new values for existing keys will replace existing ones (upsert). If no primary keys were defined, KAWA will have introduced an auto increment indicator. \n",
    "\n",
    "In that case, the incoming data will be appended to the existing ones without any replacement.\n",
    "If set to `True`, the data of the new dataframe will replace whatever was there before.\n",
    "\n",
    "#### Automatic sheet creation\n",
    "\n",
    "This is linked to the `create_sheet` input. If set to `True`, a sheet will be created after the load. Its URL will be printed in the standard output. By default, this is `False`.\n",
    "\n",
    "\n",
    "#### Upload speed\n",
    "\n",
    "In order to speed up the load, increase the `nb_threads` parameter. Do not specify a number above the number of cores of the Clickhouse server. In general, values higher than 10 won’t add anything to the loading speed. It is discouraged to use values above 10 for this parameter.\n",
    "\n",
    "When this parameter has a value above 1, the dataframe will be split into `nb_threads` parquet files and each one will be sent to the server in a separate thread. This allows to use multiple cores when streaming data into the warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935c606-00cd-4bb7-850b-87218a00adc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{\n",
    "       'id': 1,\n",
    "       'flag':True,\n",
    "       'comment':'bar',\n",
    "       'price': 1.124,\n",
    "       'order_date': datetime.date(2035,1,1),\n",
    "       'client':'Wayne Enterprises'\n",
    "}])\n",
    "\n",
    "loader = kawa.new_data_loader(df=df, datasource_name='Sample Datasource')\n",
    "\n",
    "# The two following lines are indempotent\n",
    "loader.create_datasource()\n",
    "loader.add_new_indicators_to_datasource();\n",
    "\n",
    "# Loads the content of df into the datasource 'Sample Datasource'\n",
    "loader.load_data(\n",
    "    reset_before_insert=True,\n",
    "    create_sheet=True,\n",
    "    nb_threads=1\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd0a76-cd7c-4e77-b777-4c291c5ba005",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Indexation and Partitioning\n",
    "\n",
    "When your data is stored on Clickhouse, you can use the Python API to index it and configure how it is partitionned.\n",
    "Here is a comprehensive documentation that explains the architecture of Clickhouse indexes:\n",
    "\n",
    "https://clickhouse.com/docs/en/optimize/sparse-primary-indexes\n",
    "\n",
    "\n",
    "KAWA API exposes one command to let you change the primary keys and the partition key of your table.\n",
    "\n",
    "\n",
    "At any point, you can check the schema of your Clickhouse table by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b9f8a-c9d8-43a1-9afb-4b9cab039e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasource_id = kawa.entities.datasources().get_entity_id('Sample Datasource')\n",
    "\n",
    "schema = kawa.entities.get_datasource_schema(datasource_id)\n",
    "print(schema.get('schema').replace('\\\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e051ab7-2b46-4b39-b0f3-7daaef5e4cf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Primary keys\n",
    "\n",
    "KAWA will always use the primary key set as the order key set in Clickhouse. This means that the data will be sorted by each of the primary key, in THAT order.\n",
    "\n",
    "This will result in higher performances for queries with filters on the first primary key column. The further down a column is, the less significant the speedup will be by filtering on that column.\n",
    "\n",
    "\n",
    "---\n",
    "**⚠️ WARNING**\n",
    "\n",
    "It is recommended to put in the first position the dimension on which most of the views are filtered - a date for instance. ['date','portfolio','record_id']\n",
    "\n",
    "If a data source contains a natural hierarchy, putting the hierarchy in the correct order is also a good idea: ['region','country','city','record_id']\n",
    "\n",
    "To avoid losing data, \n",
    "__THE PRIMARY KEY THAT ENSURES UNICITY MUST ALWAYS BE IN THE PROVIDED LIST__.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Partition key\n",
    "\n",
    "This will tell clickhouse how to partition the data on the hard drive. It will improve performance for ingestion and select queries. Please refer to this link for more details about partitions:\n",
    "\n",
    "https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/custom-partitioning-key\n",
    "\n",
    "Setting a partition key will also make it possible to cleanly remove old / unwanted partitions through the KAWA API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876cbb4-2549-47d8-b301-4856c614c794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Command to set a new set of primary keys and a partition \n",
    "datasource_id = kawa.entities.datasources().get_entity_id('Sample Datasource')\n",
    "\n",
    "cmd.replace_datasource_primary_keys(\n",
    "   datasource=datasource_id,\n",
    "    \n",
    "   # This will define the new set of primary keys.\n",
    "   # Please make sure that this array reflects the unicity of your records.\n",
    "   # Failing to do so will result in data loss.\n",
    "   new_primary_keys=['order_date','id'],\n",
    "    \n",
    "   # This will define the partition key, optional\n",
    "   partition_key='order_date',\n",
    "    \n",
    "   # Popular values for this are: YEAR, YEAR_AND_MONTH, YEAR_AND_WEEK\n",
    "   # This is optional\n",
    "   partition_sampler='YEAR_AND_MONTH',\n",
    "    \n",
    ");\n",
    "\n",
    "\n",
    "schema = kawa.entities.get_datasource_schema(datasource_id)\n",
    "print(schema.get('schema').replace('\\\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef6459-4392-4dff-84f5-12f534c7acba",
   "metadata": {},
   "source": [
    "## 5. Removing data from a Datasource\n",
    "\n",
    "KAWA supports two ways to remove data from datasources.\n",
    "\n",
    "- One is based on dropping entire paritions (this is the recommmended way)\n",
    "- The other one directly leverages the DELETE statement.\n",
    "\n",
    "Running those commands require the data admin privilege in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff34b4d-735f-4dc9-b383-593c6a0fce45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Command to drop a partition of data\n",
    "# Only partitions on date columns are supported for such drops (and without sampling)\n",
    "\n",
    "datasource = kawa.entities.datasources().get_entity('Sample Datasource')\n",
    "\n",
    "cmd.drop_date_partition(\n",
    "   datasource=datasource, \n",
    "   date_partition=datetime.date(2024, 1, 1)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be0a11e-c72d-4f8c-9264-3026c04a2303",
   "metadata": {},
   "source": [
    "The command below deletes data based on a series of where_clauses.\n",
    "It deletes the data that MATCHES the clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7d4f7-a682-42b2-8472-7730b47a1cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasource = kawa.entities.datasources().get_entity('Sample Datasource')\n",
    "\n",
    "# Please refer to 03_compute_notebook for the syntax here.\n",
    "# It is identical to the one for the filter operators\n",
    "where_clauses = [\n",
    "    K.where('client').in_list(['Wonka']),\n",
    "    K.where('order_date').date_range(from_inclusive=datetime.date(2024, 1, 1))\n",
    "]\n",
    "\n",
    "# Will delete all records with client = Wonka and order date from the 1st of January 2024.\n",
    "cmd.delete_data(\n",
    "    datasource=datasource, \n",
    "    delete_where=where_clauses,\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
